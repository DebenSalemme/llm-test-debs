{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DebenSalemme/llm-test-debs/blob/main/training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BA_qmuz0lZ8h"
      },
      "source": [
        "# Phi-3.5-mini-ITA fine-tuning notebook\n",
        "\n",
        "In this notebook, we fine-tune Phi-3.5-mini-instruct on a good mix of English and Italian to improve performance in Italian.\n",
        "We use [Spectrum](https://arxiv.org/abs/2406.06623) to selectively train the most informative layers of the model.\n",
        "\n",
        "**ðŸ‘£ For a complete walk-through of the fine-tuning process, check out the [accompanying article](https://huggingface.co/blog/anakin87/spectrum).**\n",
        "\n",
        "\n",
        "- [ðŸªª fine-tuned model: Phi-3.5-mini-ITA](https://huggingface.co/anakin87/Phi-3.5-mini-ITA)\n",
        "- [ðŸ’¬ðŸ‡®ðŸ‡¹ Chat with the model](https://huggingface.co/spaces/anakin87/Phi-3.5-mini-ITA)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fMXKeLxXlZ8r"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eDj1Lg8ivfia",
        "outputId": "56df14db-7887-4d5d-b222-43d049e905eb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (2.21.0)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.42.4)\n",
            "Requirement already satisfied: trl in /usr/local/lib/python3.10/dist-packages (0.10.1)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (0.32.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (1.13.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.15.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.26.4)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (17.0.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.1.4)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.5)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.6.1,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.6.1,>=2023.1.0->datasets) (2024.6.1)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.10.5)\n",
            "Requirement already satisfied: huggingface-hub>=0.21.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.23.5)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.5.15)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.4)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: torch>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from trl) (2.4.0+cu121)\n",
            "Requirement already satisfied: tyro>=0.5.11 in /usr/local/lib/python3.10/dist-packages (from trl) (0.8.10)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.2->datasets) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.8)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2024.7.4)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.4.0->trl) (1.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.4.0->trl) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.4.0->trl) (3.1.4)\n",
            "Requirement already satisfied: docstring-parser>=0.16 in /usr/local/lib/python3.10/dist-packages (from tyro>=0.5.11->trl) (0.16)\n",
            "Requirement already satisfied: rich>=11.1.0 in /usr/local/lib/python3.10/dist-packages (from tyro>=0.5.11->trl) (13.8.0)\n",
            "Requirement already satisfied: shtab>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from tyro>=0.5.11->trl) (1.7.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=11.1.0->tyro>=0.5.11->trl) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=11.1.0->tyro>=0.5.11->trl) (2.16.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.4.0->trl) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.4.0->trl) (1.3.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=11.1.0->tyro>=0.5.11->trl) (0.1.2)\n",
            "Requirement already satisfied: ninja in /usr/local/lib/python3.10/dist-packages (1.11.1.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (24.1)\n",
            "Requirement already satisfied: flash-attn in /usr/local/lib/python3.10/dist-packages (2.6.3)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from flash-attn) (2.4.0+cu121)\n",
            "Requirement already satisfied: einops in /usr/local/lib/python3.10/dist-packages (from flash-attn) (0.8.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn) (3.15.4)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn) (1.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn) (2024.6.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->flash-attn) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->flash-attn) (1.3.0)\n",
            "Requirement already satisfied: wandb in /usr/local/lib/python3.10/dist-packages (0.17.8)\n",
            "Requirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.10/dist-packages (from wandb) (8.1.7)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (0.4.0)\n",
            "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.1.43)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.10/dist-packages (from wandb) (4.2.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,<6,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.20.3)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from wandb) (6.0.2)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.32.3)\n",
            "Requirement already satisfied: sentry-sdk>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.13.0)\n",
            "Requirement already satisfied: setproctitle in /usr/local/lib/python3.10/dist-packages (from wandb) (1.3.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb) (71.0.4)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.11)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.8)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2024.7.4)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.1)\n"
          ]
        }
      ],
      "source": [
        "! pip install datasets transformers trl accelerate scipy\n",
        "! pip install ninja packaging\n",
        "! MAX_JOBS=6 pip install flash-attn --no-build-isolation --upgrade\n",
        "! pip install wandb"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nFjK29WolZ8v"
      },
      "source": [
        "## Data preparation\n",
        "\n",
        "The datasets used have different formats.\n",
        "We prepare and mix them in a single dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zTrlFStU6dBd"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset, Dataset, concatenate_datasets\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "\n",
        "# Load and process FineTome dataset\n",
        "finetome_ds = load_dataset(\"mlabonne/FineTome-100k\")[\"train\"]\n",
        "mapping_keys, mapping_values = {\"from\": \"role\", \"value\": \"content\"}, {\"human\": \"user\", \"gpt\": \"assistant\"}\n",
        "\n",
        "def process_conversation(row):\n",
        "    conv = row[\"conversations\"]\n",
        "    new_conv = [{mapping_keys[k]: mapping_values.get(v, v) for k, v in msg.items()} for msg in conv]\n",
        "    return {\"conversations\": new_conv}\n",
        "\n",
        "finetome_ds = Dataset.from_list([process_conversation(row) for row in finetome_ds])\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/Phi-3.5-mini-instruct\", trust_remote_code=True)\n",
        "\n",
        "def apply_template(examples):\n",
        "    text = [tokenizer.apply_chat_template(msg, tokenize=False, add_generation_prompt=False) for msg in examples[\"conversations\"]]\n",
        "    return {\"text\": text}\n",
        "\n",
        "finetome_ds = finetome_ds.map(apply_template, batched=True).remove_columns(\"conversations\").shuffle(seed=42)\n",
        "finetome_ds = finetome_ds.add_column(\"origin\", [\"finetome\"] * len(finetome_ds))\n",
        "\n",
        "# Load and process Capybara-Claude dataset\n",
        "capyclaude_ds = load_dataset(\"efederici/capybara-claude-15k-ita\", split=\"train\")\n",
        "capyclaude_ds = capyclaude_ds.map(apply_template, batched=True).remove_columns([\"conversations\", \"hash\"]).shuffle(seed=42)\n",
        "capyclaude_ds = capyclaude_ds.add_column(\"origin\", [\"capyclaude\"] * len(capyclaude_ds))\n",
        "\n",
        "# Concatenate and split datasets\n",
        "mixed_ds = concatenate_datasets([finetome_ds, capyclaude_ds]).shuffle(seed=42)\n",
        "mixed_ds = mixed_ds.class_encode_column(\"origin\").train_test_split(test_size=0.005, stratify_by_column=\"origin\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9a97N_QOpgom",
        "outputId": "c9bff595-08e9-4112-dacc-9af3d5dac655"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['text', 'origin'],\n",
              "        num_rows: 114106\n",
              "    })\n",
              "    test: Dataset({\n",
              "        features: ['text', 'origin'],\n",
              "        num_rows: 574\n",
              "    })\n",
              "})"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "mixed_ds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AArHitC3lZ81"
      },
      "outputs": [],
      "source": [
        "# print(mixed_ds[\"train\"][587][\"text\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nn_xg7MxlZ82"
      },
      "source": [
        "We can then check how many examples will be truncated if we choose a maximum length of X tokens (2048 in this case)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hQlIBp_LqiEs"
      },
      "outputs": [],
      "source": [
        "# from scipy.stats import percentileofscore\n",
        "# import multiprocessing\n",
        "\n",
        "# def calculate_lengths(batch):\n",
        "#     return {\"conv_lengths\": [len(tokenizer(text)[\"input_ids\"]) for text in batch[\"text\"]]}\n",
        "\n",
        "# conv_lengths = mixed_ds[\"train\"].map(\n",
        "#     calculate_lengths,\n",
        "#     batched=True,\n",
        "#     batch_size=1000,\n",
        "#     num_proc=multiprocessing.cpu_count()\n",
        "# )[\"conv_lengths\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-vHKNQI5rgZU"
      },
      "outputs": [],
      "source": [
        "# chosen_length=2048\n",
        "\n",
        "# percentile = percentileofscore(conv_lengths, chosen_length)\n",
        "# print(percentile)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JCEfSYshlZ87"
      },
      "source": [
        "## Load model\n",
        "\n",
        "For Spectrum, we need to load the model using Transformers, no quantization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WU9LxPhtyxgm"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForCausalLM\n",
        "import torch\n",
        "\n",
        "model_id = \"microsoft/Phi-3.5-mini-instruct\"\n",
        "\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    \"microsoft/Phi-3.5-mini-instruct\",\n",
        "    use_cache=False,\n",
        "    torch_dtype=torch.bfloat16,\n",
        "    attn_implementation=\"flash_attention_2\",\n",
        "    device_map=\"auto\",\n",
        "    trust_remote_code=True\n",
        ")\n",
        "\n",
        "# reference: https://huggingface.co/microsoft/Phi-3.5-mini-instruct/blob/main/sample_finetune.py\n",
        "# keep in mind that setting tokenizer.model_max_length = 2048 as suggested is WRONG\n",
        "tokenizer.pad_token = tokenizer.unk_token  # use unk rather than eos token to prevent endless generation\n",
        "tokenizer.pad_token_id = tokenizer.convert_tokens_to_ids(tokenizer.pad_token)\n",
        "tokenizer.padding_side = 'right'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iMRVyPaFlZ8-"
      },
      "source": [
        "## Apply Spectrum\n",
        "https://github.com/cognitivecomputations/spectrum\n",
        "https://arxiv.org/abs/2406.06623\n",
        "\n",
        "In short, when using Spectrum, we only fine-tune some layers of the model with high Signal-to-Noise Ratio.\n",
        "So, we need to freeze the other layers before training.\n",
        "\n",
        "---\n",
        "\n",
        "I computed the following YAML file using the Spectrum script, which unfortunately is not compatible with notebook environments.\n",
        "\n",
        "```bash\n",
        "# installation\n",
        "git clone https://github.com/cognitivecomputations/spectrum.git\n",
        "cd spectrum\n",
        "pip install -r requirements.txt\n",
        "\n",
        "# run\n",
        "python spectrum.py --model-name microsoft/Phi-3.5-mini-instruct --top-percent 30\n",
        "```\n",
        "\n",
        "This command first scans the model (if not available) and then produces the YAML file with top SNR layers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lHYDo5gAlZ8-"
      },
      "outputs": [],
      "source": [
        "# For simplicity, I'm pasting the YAML parameters here\n",
        "\n",
        "yaml_parameters=\"\"\"unfrozen_parameters:\n",
        "- ^lm_head.weight$\n",
        "- ^model.embed_tokens.weight$\n",
        "# mlp.down_proj layers\n",
        "- model.layers.2.mlp.down_proj\n",
        "- model.layers.3.mlp.down_proj\n",
        "- model.layers.1.mlp.down_proj\n",
        "- model.layers.23.mlp.down_proj\n",
        "- model.layers.4.mlp.down_proj\n",
        "- model.layers.26.mlp.down_proj\n",
        "- model.layers.25.mlp.down_proj\n",
        "- model.layers.24.mlp.down_proj\n",
        "- model.layers.28.mlp.down_proj\n",
        "# mlp.gate_up_proj layers\n",
        "- model.layers.31.mlp.gate_up_proj\n",
        "- model.layers.4.mlp.gate_up_proj\n",
        "- model.layers.3.mlp.gate_up_proj\n",
        "- model.layers.5.mlp.gate_up_proj\n",
        "- model.layers.6.mlp.gate_up_proj\n",
        "- model.layers.2.mlp.gate_up_proj\n",
        "- model.layers.30.mlp.gate_up_proj\n",
        "- model.layers.9.mlp.gate_up_proj\n",
        "- model.layers.28.mlp.gate_up_proj\n",
        "# self_attn.o_proj layers\n",
        "- model.layers.0.self_attn.o_proj\n",
        "- model.layers.1.self_attn.o_proj\n",
        "- model.layers.10.self_attn.o_proj\n",
        "- model.layers.11.self_attn.o_proj\n",
        "- model.layers.9.self_attn.o_proj\n",
        "- model.layers.3.self_attn.o_proj\n",
        "- model.layers.19.self_attn.o_proj\n",
        "- model.layers.8.self_attn.o_proj\n",
        "- model.layers.4.self_attn.o_proj\n",
        "# self_attn.qkv_proj layers\n",
        "- model.layers.23.self_attn.qkv_proj\n",
        "- model.layers.24.self_attn.qkv_proj\n",
        "- model.layers.22.self_attn.qkv_proj\n",
        "- model.layers.26.self_attn.qkv_proj\n",
        "- model.layers.27.self_attn.qkv_proj\n",
        "- model.layers.25.self_attn.qkv_proj\n",
        "- model.layers.28.self_attn.qkv_proj\n",
        "- model.layers.29.self_attn.qkv_proj\n",
        "- model.layers.31.self_attn.qkv_proj\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jh0VnnsolZ8_"
      },
      "outputs": [],
      "source": [
        "unfrozen_parameters = []\n",
        "for line in yaml_parameters.splitlines():\n",
        "  if line.startswith(\"- \"):\n",
        "    unfrozen_parameters.append(line.split(\"- \")[1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MoBVuartlZ9A"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "def _freeze_and_unfreeze_parameters(model, unfrozen_parameters):\n",
        "    # freeze all parameters\n",
        "    for param in model.parameters():\n",
        "        param.requires_grad = False\n",
        "    # unfreeze Spectrum parameters\n",
        "    for name, param in model.named_parameters():\n",
        "        if any(re.match(unfrozen_param, name) for unfrozen_param in unfrozen_parameters):\n",
        "            param.requires_grad = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uwDOYW0glZ9A"
      },
      "outputs": [],
      "source": [
        "_freeze_and_unfreeze_parameters(model, unfrozen_parameters)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "odGbF_zelZ9B"
      },
      "outputs": [],
      "source": [
        "# check the outcome of our freezing operation\n",
        "for name, param in model.named_parameters():\n",
        "    if param.requires_grad:\n",
        "      print(name, param.requires_grad)\n",
        "\n",
        "# model.embed_tokens.weight True\n",
        "# model.layers.0.self_attn.o_proj.weight True\n",
        "# model.layers.1.self_attn.o_proj.weight True\n",
        "# model.layers.1.mlp.down_proj.weight True\n",
        "# ..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CTJApzP2lZ9B"
      },
      "source": [
        "## Training configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7Qmri7QQlZ9B"
      },
      "outputs": [],
      "source": [
        "# WANDB configuration (optional)\n",
        "\n",
        "# import wandb\n",
        "# run = wandb.init(...)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3_j_YeY2roJk"
      },
      "outputs": [],
      "source": [
        "from trl import SFTConfig, SFTTrainer\n",
        "\n",
        "new_model_id=\"anakin87/Phi-3.5-mini-ITA\"\n",
        "\n",
        "cfg = SFTConfig(\n",
        "    output_dir='./mymodel',\n",
        "    overwrite_output_dir = True,\n",
        "    hub_model_id=new_model_id,\n",
        "    hub_strategy=\"every_save\",\n",
        "    save_strategy=\"steps\",\n",
        "    save_steps=500,\n",
        "    save_total_limit=1,\n",
        "    push_to_hub=True,\n",
        "    logging_steps=20,\n",
        "    max_seq_length=2048,                    # see above in \"Data preparation\" section\n",
        "    dataset_text_field=\"text\",              # since we already prepared the dataset, let's point the Trainer to the correct column\n",
        "    remove_unused_columns=True,\n",
        "    packing=True,                           # speeds up training. https://huggingface.co/docs/trl/en/sft_trainer#packing-dataset--constantlengthdataset-\n",
        "    num_train_epochs=2,\n",
        "    lr_scheduler_type=\"cosine\",\n",
        "    warmup_ratio=0.2,\n",
        "    bf16=True,\n",
        "    tf32=True,\n",
        "    learning_rate=5.0e-06,                  # suggested in https://huggingface.co/microsoft/Phi-3.5-mini-instruct/blob/main/sample_finetune.py\n",
        "    per_device_train_batch_size=8,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8eXWxsHHy4Cc"
      },
      "outputs": [],
      "source": [
        "sft_trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    args=cfg,\n",
        "    train_dataset=mixed_ds[\"train\"],\n",
        "\n",
        "    tokenizer=tokenizer\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "auUi2IXC0mxL"
      },
      "outputs": [],
      "source": [
        "sft_trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZZipjXh2lZ9D"
      },
      "outputs": [],
      "source": [
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer.pad_token_id = tokenizer.convert_tokens_to_ids(tokenizer.eos_token)\n",
        "tokenizer.padding_side = 'left'\n",
        "\n",
        "tokenizer.push_to_hub(new_model_id)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YyGlITzglZ9E"
      },
      "source": [
        "I finally did some manual updates on the model repo:\n",
        "- copying some files from the original model to my model...\n",
        "- modifying config.json and generation_config.json to use the right tokens ids for `eos_token_id`."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}